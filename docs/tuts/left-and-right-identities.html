
<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <!-- The above 3 meta tags *must* come first in the head; any other head content must come *after* these tags -->
    <title>ProvingGround</title>
    <link rel="icon" href="../IIScLogo.jpg">

    <!-- Bootstrap -->
    <link href="../css/bootstrap.min.css" rel="stylesheet">
   <link href="../css/katex.min.css" rel="stylesheet">
   <link href="../css/main.css" rel="stylesheet">


    <link rel="stylesheet" href="../css/zenburn.css">
    <script src="../js/highlight.pack.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>

   <script src="../js/ace.js"></script>
   <script src="../js/katex.min.js"></script>

    
<!-- mathjax config similar to math.stackexchange -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
jax: ["input/TeX", "output/HTML-CSS"],
tex2jax: {
  inlineMath: [ ['$', '$'] ],
  displayMath: [ ['$$', '$$']],
  processEscapes: true,
  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
},
messageStyle: "none",
"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"] }
});
</script>
<script type="text/javascript" async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_CHTML">
       </script>
    
  </head>

   
<body>
<nav class="navbar navbar-default">
      <div class="container-fluid">
        <!-- Brand and toggle get grouped for better mobile display -->
        <div class="navbar-header">
          <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1" aria-expanded="false">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
            <span class="icon-bar"></span>
          </button>
          <span class="navbar-brand">ProvingGround</span>
        </div>

        <!-- Collect the nav links, forms, and other content for toggling -->
        <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
          <ul class="nav navbar-nav" id="left-nav">
            <li><a href="../index.html">Docs Home</a></li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Tutorials (notes)<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="../tuts/hott.html">HoTT</a></li><li><a href="../tuts/left-and-right-identities.html">Left and Right Identities</a></li><li><a href="../tuts/internal-repetition-for-length-functions.html">Internal repetition for length functions</a></li><li><a href="../tuts/inductive-types-(old-style).html">Inductive Types (old style)</a></li><li><a href="../tuts/inductive-types.html">Inductive Types</a></li><li><a href="../tuts/scalarep.html">ScalaRep</a></li><li><a href="../tuts/symbolic-algebra.html">Symbolic algebra</a></li>
              </ul>
            </li>
            <li class="dropdown">
              <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">
                Posts<span class="caret"></span></a>
              <ul class="dropdown-menu">
                <li><a href="../posts/computer-assistance-in-homogenous-length-functions.html">nullComputer assistance in Homogenous length functions</a></li><li><a href="../posts/tuning-with-tensorflow.html">nullTuning with Tensorflow</a></li><li><a href="../posts/lean-import-and-propositions.html">nullLean Import and Propositions</a></li><li><a href="../posts/levels-of-parsimony,-as-seen-from-logic-runs.html">nullLevels of parsimony, as seen from Logic runs</a></li><li><a href="../posts/improvements-to-searching-and--exploration.html">nullImprovements to searching and  exploration</a></li><li><a href="../posts/logic-and-modus-ponens.html">nullLogic and Modus Ponens</a></li><li><a href="../posts/prover-components-and-identities-in-a-monoid.html">nullProver Components and Identities in a Monoid</a></li><li><a href="../posts/on-import-from-lean-export-format.html">nullOn import from Lean Export format</a></li><li><a href="../posts/&quot;quasi-literate-programming&quot;.html">null&quot;Quasi-literate programming&quot;</a></li><li><a href="../posts/stalling-in-the-lean-import---the-problem-case..html">nullStalling in the lean import - the problem case.</a></li>
              </ul>
            </li>
          </ul>
          <ul class="nav navbar-nav navbar-right">
            <li> <a href="../scaladoc/provingground/index.html" target="_blank">ScalaDocs</a></li>
            <li> <a href="https://github.com/siddhartha-gadgil/ProvingGround" target="_blank">
              <img src="../GitHub-Mark-Light-32px.png" alt="Github"></img> </a> </li>



          </ul>
        </div><!-- /.navbar-collapse -->
      </div><!-- /.container-fluid -->
    </nav>
<div class="container">
<h1 class="text-center">Left and Right Identities</h1>

<div class="text-justify">
<p>We demonstrate how the system can discover a simple proof by purely forward reasoning. The steps here are tailor-made for this proof,
and in practice there will be many blind allies.</p>
<p>We start with some axioms for a Monoid, except that the left and right identities are not assumed equal. We show that they are equal.</p>
<p>First, some imports</p>
<pre><code class="language-scala">scala&gt;        

scala&gt; import provingground.{FiniteDistribution =&gt; FD, ProbabilityDistribution =&gt; PD, _} 

import provingground.{FiniteDistribution =&gt; FD, ProbabilityDistribution =&gt; PD, _}

scala&gt; import library._, MonoidSimple._ 

import library._, MonoidSimple._

scala&gt; import learning._ 

import learning._
</code></pre>
<p>A Monoid is axiomatized as a type <code>M</code> with equality <code>eqM</code> and an operation <code>op := _*_</code>. We consider the relevant objects and axioms.</p>
<pre><code class="language-scala">scala&gt; dist1.entropyVec.mkString(&quot;\n&quot;, &quot;\n&quot;, &quot;\n&quot;) 

res7: String = &quot;&quot;&quot;
Weighted(_*_,1.5849625007211563)
Weighted(eqM,1.5849625007211563)
Weighted(axiom[eqM(_*_(a)(e_r))(a)],4.392317422778761)
Weighted(axiom[eqM(a)(a)],4.392317422778761)
Weighted(axiom[(eqM(a)(b) → eqM(b)(a))],4.392317422778761)
Weighted(e_l,4.392317422778761)
Weighted(axiom[eqM(_*_(e_l)(a))(a)],4.392317422778761)
Weighted(e_r,4.392317422778761)
Weighted(axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))],4.392317422778761)
&quot;&quot;&quot;
</code></pre>
<p>We use a <code>TermEvolver</code>, We first generate the types, which includes the theorems.
The time has not been optimized here to avoid accidental biases.</p>
<pre><code class="language-scala">scala&gt; val tv = new TermEvolver(lambdaWeight = 0.0, piWeight = 0.0) 

tv: TermEvolver = provingground.learning.TermEvolver@52f713f2

scala&gt; val fdT = Truncate(tv.baseEvolveTyps(dist1), math.pow(0.1, 8)) 

fdT: FiniteDistribution[HoTT.Typ[HoTT.Term]] = FiniteDistribution(
  Vector(
    Weighted(
      ((eqM) (((_*_) (e_l)) (e_l))) (((_*_) (((_*_) (e_r)) (e_r))) (e_l)),
      1.9112879403319085E-5
    ),
    Weighted(((eqM) (((_*_) (e_l)) (e_l))) (((_*_) (e_r)) (e_l)), 0.0010982705587242983),
    Weighted(
      ((eqM) (((_*_) (e_l)) (e_l))) (((_*_) (((_*_) (e_l)) (e_l))) (e_l)),
      1.9112879403319085E-5
    ),
    Weighted(((eqM) (((_*_) (e_l)) (e_r))) (((_*_) (e_r)) (e_r)), 0.0010982705587242983),
    Weighted(
      ((eqM) (e_r)) (((_*_) (((_*_) (e_r)) (e_r))) (((_*_) (e_l)) (e_l))),
      9.113892294053883E-6
    ),
    Weighted(
      ((eqM) (((_*_) (e_r)) (e_l))) (((_*_) (((_*_) (e_r)) (e_l))) (e_r)),
      1.9112879403319085E-5
    ),
    Weighted(((eqM) (((_*_) (e_l)) (e_r))) (((_*_) (e_r)) (e_l)), 0.0010982705587242983),
    Weighted(
      ((eqM) (e_l)) (((_*_) (e_r)) (((_*_) (e_r)) (((_*_) (e_r)) (e_r)))),
      2.208091209772826E-6
    ),
    Weighted(
      ((eqM) (((_*_) (e_r)) (e_r))) (((_*_) (e_l)) (((_*_) (e_r)) (e_l))),
      1.3226352305986202E-5
    ),
    Weighted(((eqM) (e_l)) (((_*_) (e_l)) (((_*_) (e_l)) (e_r))), 2.2243076188824677E-4),
    Weighted(
      ((eqM) (e_l)) (((_*_) (e_l)) (((_*_) (((_*_) (e_l)) (e_r))) (e_l))),
      5.863557578778798E-7
    ),
    Weighted(
      ((eqM) (((_*_) (e_l)) (((_*_) (e_l)) (e_l)))) (((_*_) (e_r)) (e_l)),
      1.0504450768866325E-5
    ),
    Weighted(
      ((eqM) (e_l)) (((_*_) (e_l)) (((_*_) (e_l)) (((_*_) (e_r)) (e_r)))),
      2.208091209772826E-6
    ),
    Weighted(((eqM) (e_r)) (e_r), 0.16770029826173272),
    Weighted(((eqM) (((_*_) (e_l)) (e_l))) (((_*_) (e_l)) (e_r)), 0.0010982705587242983),
    Weighted(
      ((eqM) (((_*_) (((_*_) (e_l)) (e_l))) (e_l))) (((_*_) (e_l)) (e_r)),
      3.003288307213677E-6
    ),
    Weighted(
...
</code></pre>
<p>We shall generate terms. Some experiments show that it is enough to generate with truncation <code>10^{-5}</code>.</p>
<pre><code class="language-scala">scala&gt; val fd = Truncate(tv.baseEvolve(dist1), math.pow(0.1, 5)) 

fd: FiniteDistribution[HoTT.Term] = FiniteDistribution(
  Vector(
    Weighted(axiom[eqM(_*_(a)(e_r))(a)], 0.03857142857142857),
    Weighted(((eqM) (e_r)) (e_r), 0.003806821968151044),
    Weighted((eqM) (((_*_) (e_r)) (e_r)), 8.855801048572368E-4),
    Weighted(((_*_) (e_r)) (e_l), 0.0038068219681510434),
    Weighted(axiom[eqM(a)(a)], 0.03857142857142857),
    Weighted(((eqM) (e_r)) (e_l), 0.003806821968151044),
    Weighted(axiom[(eqM(a)(b) → eqM(b)(a))], 0.03857142857142857),
    Weighted((_*_) (((_*_) (e_l)) (e_l)), 8.855801048572368E-4),
    Weighted(_*_, 0.27),
    Weighted(((eqM) (e_l)) (e_l), 0.0038068219681510443),
    Weighted((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_r), 0.003940271597629894),
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_r)) (e_r)) ((axiom[eqM(a)(a)]) (e_r)),
      2.4929185278420887E-4
    ),
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_l)) (e_l)) ((axiom[eqM(a)(a)]) (e_l)),
      2.4929185278420887E-4
    ),
    Weighted(((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_r)) (e_l), 5.438317097358634E-4),
    Weighted((eqM) (e_r), 0.02642897788208435),
    Weighted((axiom[eqM(_*_(e_l)(a))(a)]) (e_l), 0.003940271597629894),
    Weighted(((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (e_r), 5.438317097358635E-4),
    Weighted((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l), 0.003940271597629894),
    Weighted(
      ($cvhue :  M) ↦ (((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (e_l)) ($cvhue)) ((axiom[eqM(a)(a)]) (e_l))),
      2.4929185278420887E-4
    ),
    Weighted(
      ($cvhsc :  M) ↦ (((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (((_*_) (e_l)) (e_r))) (e_r)) ($cvhsc)) ((axiom[eqM(_*_(e_l)(a))(a)]) (e_r))),
      2.4929185278420887E-4
    ),
    Weighted((eqM) (((_*_) (e_r)) (e_l)), 8.855801048572368E-4),
    Weighted(e_l, 0.03857142857142857),
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_l))) (e_l)) ((axiom[eqM(_*_(e_l)(a))(a)]) (e_l)),
      2.4929185278420887E-4
    ),
    Weighted(((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_l)) (e_l), 5.438317097358635E-4),
    Weighted((_*_) (((_*_) (e_r)) (e_r)), 8.855801048572368E-4),
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_r)) (e_r))) (e_r)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_r)),
      2.4929185278420887E-4
...

scala&gt; fd.filter(_.typ == eqM(l)(op(l)(r))) 

res11: FiniteDistribution[HoTT.Term] = FiniteDistribution(
  Vector(
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)),
      2.4929185278420887E-4
    )
  )
)
</code></pre>
<p>We see that wee get a proof of a key lemma. Criteria, based on probabilities of statements and proofs,
tell us that this is one of the best results proved, along with one related by symmetry and a pair that are not useful.</p>
<p>A quick way to explore consequences of this discovered lemma is to use the derivative of the evolution.
We see that we get the proof.</p>
<pre><code class="language-scala">scala&gt; val pf = fd.filter(_.typ == eqM(l)(op(l)(r))).supp.head 

pf: HoTT.Term = (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l))

scala&gt; val initt = TangVec(dist1, FD.unif(pf)) 

initt: TangVec[FiniteDistribution[HoTT.Term]] = TangVec(
  FiniteDistribution(
    Vector(
      Weighted(e_l, 0.047619047619047616),
      Weighted(e_r, 0.047619047619047616),
      Weighted(_*_, 0.047619047619047616),
      Weighted(eqM, 0.047619047619047616),
      Weighted(axiom[eqM(a)(a)], 0.047619047619047616),
      Weighted(axiom[(eqM(a)(b) → eqM(b)(a))], 0.047619047619047616),
      Weighted(axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))], 0.047619047619047616),
      Weighted(axiom[eqM(_*_(e_l)(a))(a)], 0.047619047619047616),
      Weighted(axiom[eqM(_*_(a)(e_r))(a)], 0.047619047619047616),
      Weighted(eqM, 0.2857142857142857),
      Weighted(_*_, 0.2857142857142857)
    )
  ),
  FiniteDistribution(
    Vector(
      Weighted(
        (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)),
        1.0
      )
    )
  )
)

scala&gt; val fdt = Truncate(tv.evolve(initt).vec , math.pow(0.1, 4)) 

fdt: FiniteDistribution[HoTT.Term] = FiniteDistribution(
  Vector(
    Weighted(
      ((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (((_*_) (e_l)) (e_r))) (e_l)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l))),
      0.06628023644707162
    ),
    Weighted(
      ((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (((_*_) (e_l)) (e_r))) (e_r)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l))),
      0.06628023644707162
    ),
    Weighted(
      ($cvply :  M) ↦ (((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (((_*_) (e_l)) (e_r))) (e_l)) ($cvply)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_l)) (((_*_) (e_l)) (e_r))) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l))))),
      0.0013066811853465038
    ),
    Weighted(
      (_ :  ((eqM) (e_r)) ($cvptj)) ↦ ((((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_r)) (e_l)) (((_*_) (e_l)) (e_r))) ($cvptt)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)))),
      0.0017240932306655259
    ),
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (e_l)) (((_*_) (e_l)) (e_r))) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)))),
      0.0013066811853465038
    ),
    Weighted(
      ($cvphh :  M) ↦ (((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (((_*_) (e_l)) (e_r))) ($cvphh)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)))),
      0.025244354566902594
    ),
    Weighted(
      (((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)),
      0.81
    ),
    Weighted(
      (((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (((_*_) (e_l)) (e_r))) (e_r)) ((((axiom[(eqM(a)(b) → eqM(b)(a))]) (((_*_) (e_l)) (e_r))) (e_l)) ((axiom[eqM(_*_(a)(e_r))(a)]) (e_l)))) ((axiom[eqM(_*_(e_l)(a))(a)]) (e_r)),
      0.0013066811853465038
    ),
    Weighted(
      (((((axiom[(eqM(a)(b) → (eqM(b)(c) → eqM(a)(c)))]) (e_l)) (((_*_) (e_l)) (e_r))) (e_l)) ((((ax...

scala&gt; val tqs = fdt.map(_.typ).filter(fdT(_) &gt; 0).flatten 

tqs: FiniteDistribution[HoTT.Typ[U] forSome { type U &gt;: x$1 &lt;: HoTT.Term with HoTT.Subs[U]; val x$1: HoTT.Term }] = FiniteDistribution(
  Vector(
    Weighted(((eqM) (((_*_) (e_l)) (e_r))) (e_l), 0.025244354566902594),
    Weighted(((eqM) (e_l)) (e_l), 0.0013066811853465038),
    Weighted(((eqM) (e_l)) (e_r), 0.0013066811853465038),
    Weighted(((eqM) (e_l)) (((_*_) (e_l)) (e_r)), 0.8113066811853465)
  )
)

scala&gt; tqs(eqM(l)(r)) 

res16: Double = 0.0013066811853465038

scala&gt;  

scala
</code></pre>
<p>The steps of the proof in this case took less than <code>0.1</code> seconds. Of course in practice we assume other axioms and follow other paths.
But hopefully the time taken is just a few seconds.</p>
<h4>Git Log when running tutorial</h4>
<pre><code>commit b2b7c9c2700c59457203b8fdd352733643df812e
Author: Siddhartha Gadgil &lt;siddhartha.gadgil@gmail.com&gt;
Date:   Wed Jun 6 10:00:45 2018 +0530

    testing monix, proving
    bizarre errors in proving, perhaps due to memoization
</code></pre>
<ul>
<li><strong>Branch</strong> : master</li>
</ul>


</div>
</div>

<div class="container-fluid">
  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <p>&nbsp;</p>
  <div class="footer navbar-fixed-bottom bg-primary">
    <h4>
    &nbsp;Developed by:
    &nbsp;<a href="http://math.iisc.ac.in/~gadgil" target="_blank">&nbsp; Siddhartha Gadgil</a>

  </h4>

  </div>
</div>
<script type="text/javascript" src="../js/jquery-2.1.4.min.js"></script>
<script type="text/javascript" src="../js/bootstrap.min.js"></script>
<script type="text/javascript" src="../js/provingground.js"></script>
<script>
  provingground.main()
</script>
   
</body>
</html>
   